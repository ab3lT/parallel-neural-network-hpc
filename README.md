# Parallel Deep Neural Network Training

<p align="center">
  <img src="https://img.shields.io/badge/Language-C99-blue.svg" alt="Language">
  <img src="https://img.shields.io/badge/Parallelization-OpenMP%20%7C%20MPI-green.svg" alt="Parallel">
  <img src="https://img.shields.io/badge/Dataset-MNIST-orange.svg" alt="Dataset">
  <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License">
</p>

<p align="center">
  <b>High-performance parallel implementation of neural network training using OpenMP, MPI, and Hybrid approaches</b>
</p>

<p align="center">
  <a href="#-features">Features</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-performance-results">Results</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-technologies">Technologies</a>
</p>

---

## ğŸ“‹ Overview

This project implements and benchmarks **parallel training algorithms** for deep neural networks from scratch in C. It demonstrates expertise in high-performance computing, parallel programming paradigms, and deep learning fundamentals.

The implementation compares four approaches:
- **Serial Baseline** - Sequential implementation for performance baseline
- **OpenMP** - Shared-memory parallelization for multi-core CPUs
- **MPI** - Distributed-memory parallelization for cluster computing
- **Hybrid MPI+OpenMP** - Two-level parallelism for maximum scalability

> ğŸ“ Developed as part of the **Distributed Computing for AI** Masters course

---

## âœ¨ Features

- ğŸ§  **From-Scratch Implementation** - Neural network built entirely in C without ML frameworks
- âš¡ **Multiple Parallelization Strategies** - OpenMP, MPI, and Hybrid implementations
- ğŸ“Š **MNIST Support** - Train on real handwritten digit dataset
- ğŸ“ˆ **Performance Analysis** - Automated benchmarking and visualization tools
- ğŸ”§ **Configurable** - Adjustable hyperparameters via command line
- ğŸ“ **Well Documented** - Comprehensive technical report and code comments

---

## ğŸ—ï¸ Architecture

### Neural Network Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT LAYER (784)                        â”‚
â”‚                   28Ã—28 MNIST Image                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HIDDEN LAYER 1 (256)                       â”‚
â”‚                    ReLU Activation                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HIDDEN LAYER 2 (128)                       â”‚
â”‚                    ReLU Activation                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OUTPUT LAYER (10)                         â”‚
â”‚                  Softmax Activation                         â”‚
â”‚                   (Digits 0-9)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Model Specifications:**
| Component | Details |
|-----------|---------|
| Parameters | ~235,000 trainable weights |
| Loss Function | Cross-Entropy |
| Optimizer | Mini-batch SGD |
| Weight Init | He (ReLU) / Xavier (Softmax) |

### Parallelization Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SERIAL (Baseline)                            â”‚
â”‚    [Sample 1] â†’ [Sample 2] â†’ [Sample 3] â†’ ... â†’ [Update Weights]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OpenMP (Shared Memory)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Thread 0   â”‚ â”‚  Thread 1   â”‚ â”‚  Thread 2   â”‚  ... Threads       â”‚
â”‚  â”‚  Samples    â”‚ â”‚  Samples    â”‚ â”‚  Samples    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â–¼                                            â”‚
â”‚              [Gradient Reduction + Weight Update]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MPI (Distributed Memory)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Rank 0    â”‚ â”‚   Rank 1    â”‚ â”‚   Rank 2    â”‚  ... Processes     â”‚
â”‚  â”‚ Local Data  â”‚ â”‚ Local Data  â”‚ â”‚ Local Data  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                         â–¼                                            â”‚
â”‚                  [MPI_Allreduce]                                     â”‚
â”‚              [Synchronized Weight Update]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hybrid MPI+OpenMP                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚        Node 0           â”‚    â”‚        Node 1           â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚
â”‚  â”‚  â”‚Thread0â”‚ â”‚Thread1â”‚   â”‚    â”‚  â”‚Thread0â”‚ â”‚Thread1â”‚   â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚    â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚         â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚    â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚         â”‚
â”‚  â”‚     Local Reduce       â”‚    â”‚     Local Reduce       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                            â–¼                                         â”‚
â”‚                     [MPI_Allreduce]                                  â”‚
â”‚                  [Global Weight Update]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Performance Results

### Speedup Comparison

| Implementation | Configuration | Speedup | Efficiency |
|---------------|---------------|---------|------------|
| Serial | 1 core | 1.00Ã— | 100% |
| OpenMP | 2 threads | ~1.7Ã— | 85% |
| OpenMP | 4 threads | ~3.1Ã— | 78% |
| OpenMP | 8 threads | ~4.5Ã— | 56% |
| MPI | 4 processes | ~2.8Ã— | 70% |
| Hybrid | 2Ã—2 (4 total) | ~3.2Ã— | 80% |

### Training Convergence

All parallel implementations achieve **identical convergence** to the serial baseline, validating correctness of the synchronous data-parallel approach.

<p align="center">
  <img src="plots/all_metrics.png" alt="Performance Results" width="800">
</p>

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Ubuntu/Debian
sudo apt-get install build-essential libopenmpi-dev openmpi-bin

# macOS
brew install gcc open-mpi
```

### Build

```bash
git clone https://github.com/yourusername/parallel-dnn-training.git
cd parallel-dnn-training

# Build all implementations
make all

# Or build without MPI
make no-mpi
```

### Download MNIST Dataset

```bash
chmod +x scripts/download_mnist.sh
./scripts/download_mnist.sh
```

### Run Training

```bash
# Serial baseline
./bin/serial_train --mnist data/mnist -n 10000 -e 20

# OpenMP (4 threads)
./bin/openmp_train --mnist data/mnist -n 10000 -e 20 -t 4

# MPI (4 processes)
mpirun -np 4 ./bin/mpi_train --mnist data/mnist -n 10000 -e 20

# Hybrid (2 processes Ã— 2 threads)
mpirun -np 2 ./bin/hybrid_train --mnist data/mnist -n 10000 -e 20 -t 2
```

### Generate Performance Graphs

```bash
pip install matplotlib pandas numpy
python scripts/plot_results.py
```

---

## ğŸ› ï¸ Technologies

<p align="center">
  <img src="https://img.shields.io/badge/C-00599C?style=for-the-badge&logo=c&logoColor=white" alt="C">
  <img src="https://img.shields.io/badge/OpenMP-4BA82E?style=for-the-badge" alt="OpenMP">
  <img src="https://img.shields.io/badge/MPI-003366?style=for-the-badge" alt="MPI">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/Make-A42E2B?style=for-the-badge&logo=gnu&logoColor=white" alt="Make">
</p>

| Category | Technologies |
|----------|-------------|
| **Languages** | C99, Python 3 |
| **Parallel Computing** | OpenMP 4.0+, MPI (OpenMPI/MPICH) |
| **Build System** | GNU Make, GCC |
| **Visualization** | Matplotlib, Pandas, NumPy |
| **Dataset** | MNIST (IDX format) |

---

## ğŸ“ Project Structure

```
parallel-dnn-training/
â”œâ”€â”€ ğŸ“‚ include/
â”‚   â””â”€â”€ neural_net.h          # Data structures & function prototypes
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ utils.c               # Utilities, MNIST loader, activations
â”‚   â”œâ”€â”€ serial_train.c        # Serial baseline implementation
â”‚   â”œâ”€â”€ openmp_train.c        # OpenMP parallel implementation
â”‚   â”œâ”€â”€ mpi_train.c           # MPI distributed implementation
â”‚   â””â”€â”€ hybrid_train.c        # Hybrid MPI+OpenMP implementation
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ download_mnist.sh     # MNIST dataset downloader
â”‚   â””â”€â”€ plot_results.py       # Performance visualization
â”œâ”€â”€ ğŸ“‚ plots/                  # Generated performance graphs
â”œâ”€â”€ ğŸ“‚ data/                   # Dataset directory
â”œâ”€â”€ ğŸ“„ Makefile               # Build configuration
â”œâ”€â”€ ğŸ“„ Technical_Report.pdf   # Detailed technical report
â”œâ”€â”€ ğŸ“„ Technical_Report.tex   # LaTeX source
â””â”€â”€ ğŸ“„ README.md              # This file
```

---

## ğŸ¯ Skills Demonstrated

This project showcases proficiency in:

- **High-Performance Computing (HPC)**
  - Parallel algorithm design and implementation
  - Performance optimization and benchmarking
  - Strong and weak scaling analysis

- **Parallel Programming**
  - OpenMP directives and thread management
  - MPI communication patterns (Allreduce, Barrier)
  - Hybrid parallelization strategies

- **Deep Learning Fundamentals**
  - Neural network architecture design
  - Backpropagation algorithm implementation
  - Gradient descent optimization

- **Systems Programming**
  - Low-level C programming
  - Memory management
  - Binary file I/O (MNIST IDX format)

- **Software Engineering**
  - Modular code organization
  - Build system design (Makefile)
  - Documentation and technical writing

---

## ğŸ“ˆ Future Improvements

- [ ] GPU acceleration with CUDA
- [ ] Asynchronous gradient updates
- [ ] Gradient compression techniques
- [ ] Support for convolutional layers
- [ ] Mixed-precision training (FP16)
- [ ] Docker containerization

---

## ğŸ“š References

1. Dean, J., et al. "Large Scale Distributed Deep Networks." NIPS 2012.
2. Goyal, P., et al. "Accurate, Large Minibatch SGD." arXiv 2017.
3. Ben-Nun, T., and Hoefler, T. "Demystifying Parallel and Distributed Deep Learning." ACM Computing Surveys 2019.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Your Name**

- GitHub: [@ab3lT](https://github.com/ab3lT)
- LinkedIn: [Abel Tadesse](https://www.linkedin.com/in/abeltadessealemu/)
- Email: se.abel.tadesse@gmail.com

---

<p align="center">
  <b>â­ If you found this project useful, please consider giving it a star!</b>
</p>
